{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e2473c6f",
   "metadata": {},
   "source": [
    "Why Use Activation Functions in Hidden Layers?\n",
    "In a neural network, each node in a hidden layer takes inputs from the previous layer, processes them, and passes the result forward. This processing starts with a linear transformation—a weighted sum of the inputs (plus a bias sometimes). Without an activation function, the network would only perform linear operations. Since stacking multiple linear layers still results in a single linear function, the network couldn’t model complex, non-linear relationships—like those in real-world tasks such as image or speech recognition.\n",
    "\n",
    "Activation functions add non-linearity to this process. By applying a non-linear operation to the linear transformation’s output, they enable the network to learn and approximate intricate patterns, making them essential for hidden layers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af543f71",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5395f0ea",
   "metadata": {},
   "source": [
    "Common Activation Functions and Their Use Cases\n",
    "Here are the most widely used activation functions for hidden layers, along with their roles:\n",
    "\n",
    "Sigmoid\n",
    "\n",
    "Formula: $ \\sigma(x) = \\frac{1}{1 + e^{-x}} $\n",
    "Range: (0, 1)\n",
    "Use Case: Rarely used in hidden layers due to the vanishing gradient problem (gradients become tiny, slowing learning). It’s more common in output layers for binary classification.\n",
    "\n",
    "\n",
    "Hyperbolic Tangent (Tanh)\n",
    "\n",
    "Formula: $ \\tanh(x) = \\frac{e^x - e^{-x}}{e^x + e^{-x}} $\n",
    "Range: (-1, 1)\n",
    "Use Case: Used in hidden layers as it centers data (outputs range from -1 to 1), but it also suffers from vanishing gradients.\n",
    "\n",
    "\n",
    "Rectified Linear Unit (ReLU)\n",
    "\n",
    "Formula: $ \\text{ReLU}(x) = \\max(0, x) $\n",
    "Range: [0, ∞)\n",
    "Use Case: The go-to choice for hidden layers. It’s fast and reduces vanishing gradient issues, though it can lead to “dead neurons” (stuck at zero) if inputs are consistently negative.\n",
    "\n",
    "\n",
    "Leaky ReLU\n",
    "\n",
    "Formula: $ \\text{Leaky ReLU}(x) = \\begin{cases} x & \\text{if } x > 0 \\\\ \\alpha x & \\text{otherwise} \\end{cases} $ (where $ \\alpha $ is small, e.g., 0.01)\n",
    "Range: (-∞, ∞)\n",
    "Use Case: A fix for ReLU’s dead neuron problem, allowing a small gradient for negative inputs. Great for hidden layers.\n",
    "\n",
    "\n",
    "Softmax\n",
    "\n",
    "Formula: $ \\text{softmax}(\\mathbf{z})_i = \\frac{e^{z_i}}{\\sum_{j} e^{z_j}} $\n",
    "Range: (0, 1), sums to 1\n",
    "Use Case: Not typically for hidden layers; it’s used in output layers for multi-class classification."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf456af5",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
